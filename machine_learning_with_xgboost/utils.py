from collections import Counter
from os import listdir
import csv
import pydasm
from time import time
from random import randint

INSTR_SET = {'mov', 'xchg', 'stc', 'clc', 'cmc', 'std', 'cld', 'sti', 'cli', 'push', 'pushf', 'pusha',
             'pop', 'popf', 'popa', 'ccombw', 'cwd', 'cwde', 'in', 'out', 'add', 'adc', 'sub', 'sbb',
             'div', 'idiv', 'mul', 'imul', 'inc', 'dec', 'cmp', 'sal', 'sar', 'rcl', 'rcr', 'rol', 'ror',
             'neg', 'not', 'and', 'or', 'xor', 'shl', 'shr', 'nop', 'lea', 'int', 'call', 'jmp', 'je',
             'jz', 'jcxz', 'jp', 'jpe', 'ja', 'jae', 'jb', 'jbe', 'jna', 'jnae', 'jnb', 'jnbe', 'jc',
             'jnc', 'ret', 'jne', 'jnz', 'jecxz', 'jnp', 'jpo', 'jg', 'jge', 'jl', 'jle', 'jng', 'jnge', 'jnl',
             'jnle', 'jo', 'jno', 'js', 'jns', 'jns', 'popa', 'rol', 'popf', 'jnz', 'imul', 'lds', 'jna', 'jng', 'jno',
             'jnl', 'arpl', 'cli', 'cld', 'clc', 'add', 'adc', 'scasd', 'scasb', 'daa', 'mov', 'das', 'nop', 'repne',
             'jnc', 'cmc', 'leave', 'jmpf', 'cmp', 'hlt', 'loope', 'pusha', 'pushf', 'out', 'xor', 'sub', 'rep', 'ret',
             'jecxz', 'xchg', 'cwd', 'lea', 'jz', 'jp', 'js', 'jl', 'jo', 'jg', 'ja', 'jc', 'sbb', 'sahf', 'stosb',
             'movsd', 'movsb', 'les', 'xlat', 'or', 'into', 'bound', 'pop', 'fildl', 'retf', 'retn', 'fadds', 'faddl',
             'call', 'wait', 'sldt', 'fiaddl', 'jmp', 'int1', 'int3', 'std', 'aad', 'aaa', 'stc', 'aam', 'sti', 'aas',
             'lahf', 'dec', 'loop', 'and', 'jpo', 'int', 'lock', 'in', 'flds', 'fldl', 'cbw', 'fild', 'inc', 'cmpsb',
             'callf', 'cmpsd', 'test', 'fiadd', 'stosd', 'insb', 'outsv', 'iret', 'outsb', 'insv', 'loopne', 'salc',
             'lodsb', 'lodsd', 'enter', 'push'}

UNKNOWN = "unk"
labels_file = "trainLabels.csv"
benign_label = 0
dll_disassemble_suf = ".dlldisassemble"


def get_opcodes_from_asm_file(file_path):
    """
    Reads the disassembly file and extracts all the opcodes used.
    (opcodes that counts are those in INSTR_SET)
    :param file_path: The path to the asm file
    :return: A list with all the opcodes from INSTR_SET (as str) in the file
    """
    opcodes = []
    for line in file(file_path):
        if not line.startswith(".text") and not line.startswith("CODE"):
            continue
        line = line.split()
        if len(line) < 3:  # no opcode
            continue
        for s in line[1:]:
            if s in INSTR_SET:
                opcodes.append(s)
                break
            elif s == ';':
                break

    return opcodes


def make_ngrams(opcodes, n):
    """
    Make a n-sized tuple of opcodes serially, e.g if n = 4 then (opcodes[0], opcodes[1], opcodes[2], opcodes[3]) will be
        the first, and the second will be (opcodes[1], opcodes[2], opcodes[3], opcodes[4]) and so on...
    :param opcodes: A list of opcodes
    :param n: The size of tuples to make
    :return: A list of n sized tuples
    """
    return [tuple(opcodes[i:i + n]) for i in xrange(len(opcodes) - n)]


def get_all_possible_ngrams_from_files(ngrams_lists, ns, use_unknown=True, unknown_precent=0.01):
    """
    Given all the ngrams used in all the training files, finds out which ngrams are used and gives them a unique index.
    :param ngrams_lists: A dict that maps from n to a list,  where the i-th element in the list is a list that contains
        the ngrams (of the specific n) for the i-th file.
    :type ngrams_lists: dict
    :param ns: A list of the "n"s that are used for ngrams.
    :param use_unknown: If True, then an "unknown ngram" token will be added (and given a unique index) for ngrams
        not found in the training files. Also a some of the least common ngrams will be replaced with the
        "unknown ngram" token for training purposes.
    :param unknown_precent: A number between 0 to 1, that indicates what part of all the ngrams is considered not common
    :return: ngrams_sets, i2ngram, ngram2i where:
        * ngrams_sets is a dict that maps from n to a set of all the ngrams used (n-sized)
        * i2ngram is a list of all the ngrams used (without those "unknown" to us) (name means that given an index
            return a ngram)
        * ngram2i is a dict that maps an ngram to its unique index in i2ngram list (given a ngram return index)
    """
    ngrams_lists = {n: [ngram for f in l for ngram in f] for n, l in ngrams_lists.iteritems()}
    ngrams_sets = {n: set(l) for n, l in ngrams_lists.iteritems()}

    if use_unknown:
        counters = Counter([ngram for n in ns for ngram in ngrams_lists[n]])
        not_used = counters.most_common()[:int(-unknown_precent * len(counters)) - 1:-1]
        for ngram in not_used:
            n = len(ngram[0])
            ngrams_sets[n].discard(ngram[0])

    i2ngram = [ngram for n in sorted(list(ngrams_sets.keys())) for ngram in sorted(list(ngrams_sets[n]))]
    if use_unknown:
        i2ngram.append(UNKNOWN)
    ngram2i = {ngram: i for i, ngram in enumerate(i2ngram)}
    return ngrams_sets, i2ngram, ngram2i


def make_ngram_counters_for_files(ngrams_lists, ngram_sets, i2ngram, ngram2i, ns, use_unknown=True):
    """
    For each file, counts how many time a certain ngram is used.
    :param ngrams_lists: A dict that maps from n to a list,  where the i-th element in the list is a list that contains
        the ngrams (of the specific n) for the i-th file.
    :param i2ngram: A list of all the ngrams used (without those "unknown" to us) (name means that given an index
        return a ngram)
    :param ngram2i: A dict that maps an ngram to its unique index in i2ngram list (given a ngram return index)
    :param ngram_sets: A dict that maps from n to a set of all the ngrams used (n-sized)
    :type ngram_sets: dict
    :param ns: A list of the "n"s that are used for ngrams.
    :param use_unknown: If True, then an "unknown ngram" token will be added (and given a unique index) for ngrams
        not found in the training files. Also a some of the least common ngrams will be replaced with the
        "unknown ngram" token for training purposes.
    :return: A list, where element #i is a list of counters that counts how many time an ngram is used (indexes matches
        i2ngram) for i-th file (in ngrams_lists)
    """
    num_files = len(ngrams_lists[ns[0]])
    features = []
    for i in xrange(num_files):
        file_features = [0] * len(i2ngram)
        for n in ns:
            for ngram in ngrams_lists[n][i]:
                index = ngram2i[ngram] if not use_unknown or ngram in ngram_sets[n] else ngram2i[UNKNOWN]
                file_features[index] += 1
        features.append(file_features)
    return features


def get_malware_files_features_and_labels(path_to_folder, ns, use_unknown=True, unknown_rate=0.01):
    """
    Reads the malware train files and returns their features and labels
    :param path_to_folder: The folder where the asm files and the labels file are located
    :param ns: A list of the "n"s that are used for ngrams.
    :param use_unknown: If True, then an "unknown ngram" token will be added (and given a unique index) for ngrams
        not found in the training files. Also a some of the least common ngrams will be replaced with the
        "unknown ngram" token for training purposes.
    :param unknown_rate: A number between 0 to 1, that indicates what part of all the ngrams is considered not common
    :return: features, labels, ngrams_sets, i2ngram, ngram2i, labels_dict where:
        * features is a list containing each file's features (a list matching i2ngram that counts how many time a
            ngram is used)
        * labels is a list which holds the label (0-9) of each file. Matches features (i-th element is the same file)
        * ngrams_sets is a dict that maps from n to a set of all the ngrams used (n-sized)
        * i2ngram is a list of all the ngrams used (without those "unknown" to us) (name means that given an index
            return a ngram)
        * ngram2i is a dict that maps an ngram to its unique index in i2ngram list (given a ngram return index)
        * labels_dict is a dictionary that maps between as asm file (without the ".asm" suffix) to it's labels (
            according to the labels file)
    """
    files = listdir(path_to_folder)
    ngrams_list = {n: [] for n in ns}
    labels_dict = {row[0]: row[1] for row in csv.reader(open(path_to_folder + "/" + labels_file, "rb"), delimiter=',')}
    labels = []
    for f in files:
        if f.endswith(".asm"):
            opcodes = get_opcodes_from_asm_file(path_to_folder + "/" + f)
            for n in ns:
                ngrams_list[n].append(make_ngrams(opcodes, n))
            labels.append(labels_dict[f[:-4]])

    # now we have a dictionary that maps from an n to a list of ngram partition for each file
    ngrams_sets, i2ngram, ngram2i = get_all_possible_ngrams_from_files(ngrams_list, ns, use_unknown, unknown_rate)
    features = make_ngram_counters_for_files(ngrams_list, ngrams_sets, i2ngram, ngram2i, ns, use_unknown)

    return features, labels, ngrams_sets, i2ngram, ngram2i, labels_dict


def disassemble_dll(file_path):
    f = open(file_path, "rb")
    buff = f.read()
    f.close()
    instructions = []

    offset = 0
    while offset < len(buff):
        i = pydasm.get_instruction(buff[offset:], pydasm.MODE_32)
        if not i:
            break
        instructions.append(pydasm.get_instruction_string(i, pydasm.FORMAT_INTEL, 0))
        offset += i.length
    return instructions


def get_opcodes_of_disassembly_list(disassembly):
    opcodes = []
    for instr in disassembly:
        if instr:
            split = instr.split()
            for op in split:
                if op in INSTR_SET:
                    opcodes.append(op)
                    break
    return opcodes


def disassemble_to_file(file_path, new_file_name):
    dis = disassemble_dll(file_path)
    with open(new_file_name, "w") as f:
        for instruction in dis:
            f.write(instruction + "\n")
    return dis


def get_all_files_features_and_labels(path_to_malware_folder, path_to_benign_folder, ns, use_unknown=True,
                                      unknown_rate=0.01,num_of_files = 1000):


    print("start load  data")
    # read malware
    ngrams_list = {n: [] for n in ns}
    labels_dict = {row[0]: int(row[1]) for row in
                   csv.reader(open(path_to_malware_folder + "/" + labels_file, "rb"), delimiter=',') if
                   row[1] != "Class"}
    labels = []
    counters = [0] * 10
    limit = num_of_files
    for f, c in labels_dict.iteritems():
        if counters[int(c)] < limit:
            opcodes = get_opcodes_from_asm_file(path_to_malware_folder + "/" + f + ".asm")
            for n in ns:
                ngrams_list[n].append(make_ngrams(opcodes, n))
            labels.append(int(c))
            counters[int(c)] += 1



    # read benign
    files = listdir(path_to_benign_folder)
    limit = num_of_files
    for f in files:
        if limit == 0:
            break
        if limit % 20 == 0:
            print("limit is {}".format(limit))
        if f.endswith(".bytes"):
            if f[:-6] + dll_disassemble_suf not in files:
                temp = disassemble_to_file(path_to_benign_folder + "/" + f, path_to_benign_folder + "/" + f[:-6] +
                                           dll_disassemble_suf)
            else:
                temp = [line[:-1] for line in file(path_to_benign_folder + "/" + f[:-6] + dll_disassemble_suf) if
                        line !=
                        "\n"]
            opcodes = get_opcodes_of_disassembly_list(temp)
            for n in ns:
                ngrams_list[n].append(make_ngrams(opcodes, n))
            labels.append(benign_label)
        limit -= 1

    # get features, labels and other stuff
    ngrams_sets, i2ngram, ngram2i = get_all_possible_ngrams_from_files(ngrams_list, ns, use_unknown, unknown_rate)
    features = make_ngram_counters_for_files(ngrams_list, ngrams_sets, i2ngram, ngram2i, ns, use_unknown)

    return features, labels, ngrams_sets, i2ngram, ngram2i, labels_dict


def seperate_to_train_and_test(features, labels, test_size= 0.2, num_of_classes = 10):
    test_len = int(len(features)*test_size)
    n= int(test_len/num_of_classes)
    train_ftr = list(features)
    train_lbl = list(labels)
    test_ftr = []
    test_lbl = []
    for i in range(0,num_of_classes):
        c = 0
        for x, y in zip(features, labels):
            if y == i:
                test_ftr.append(x)
                test_lbl.append(y)
                c+=1
                if c == n:
                    break
    # delete test from ftr
    for x,y in zip(test_ftr,test_lbl):
        train_ftr.remove(x)
        train_lbl.remove(y)

    # the remaining elements
    sub_len = test_len-len(test_ftr)
    for i in range(0,sub_len):
        temp = randint(0, len(train_ftr)-1)
        test_ftr.append(train_ftr[temp])
        test_lbl.append(train_lbl[temp])
        del train_ftr[temp]
        del train_lbl[temp]
    return train_ftr, test_ftr, train_lbl, test_lbl





# tests of utils functions
if __name__ == '__main__':
    start = time()
    x, y, sets, i2n, n2i, l_dict = get_all_files_features_and_labels("/media/user/New Volume/train", "/media/user/New Volume/benign", [4],num_of_files=100000)
    print len(i2n)
    print len(x[0])
    print [i for i, a in enumerate(x[0]) if a > 0]
    print y[0]
    print "took {} seconsd".format(time()-start)
    pass
